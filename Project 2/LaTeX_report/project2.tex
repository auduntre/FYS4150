\documentclass[a4paper, fontsize=11pt]{article}

\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage[english]{babel} % English language/hyphenation
%\usepackage{hyperref}
\usepackage{listings}
\usepackage{color}
\usepackage{verbatim}
\usepackage{graphicx}

\usepackage[colorlinks=true,linkcolor=black,urlcolor=blue,
citecolor=blue]{hyperref}

%\bibliographystyle{ieeetr}
\bibliographystyle{apalike}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}


\lstset{frame=tb,
  language=C++,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\begin{document}

\title{Project 2 FYS4150 \\ Eigenvalue problems}
\author{Audun Tahina Reitan and Marius Holm}

%----------------------------------------------------------------------------------------
%	PROBLEM 1
%----------------------------------------------------------------------------------------
\maketitle


\section{Abstract}



\section{Introduction}
In this project we'll develop code for solving eigenvalue problems. Our eigenvalue solver will be based on Jacobi's method, while the matrix we need to diagonalize is the tridiagonal Toeplitz matrix. This matrix has analytical eigenvalues and eigenvectors, which makes it easier for us to test our algorithms. 


\paragraph{}
The first problem we'll look at is the two-point boundary value problem of a buckling beam or a spring fastened at both ends. This problem has analytical solutions, and by adding a new variable along the diagonal we can study quantum mechanical problems. From quantum mechanics we will study the harmonic oscillator problem, with one or two electrons. For the two electron problem we can study the effects of Coulomb interaction and extract some interesting physics results.  The two electron problem even has analytical solutions for selected frequencies.\cite{PhysRevA.48.3561}

\paragraph{}
We introduce the relevant methods along with a brief explanation of our implementation.


Structure of report


\section{Methods}

\subsection{Jacobi's method}
Jacobi's method is a simple method for solving linear algebra problems of the sort

\begin{equation}
\hat{A} \textbf{x} = \textbf{b}
\end{equation}

where $\hat{A}$ is a matrix and \textbf{x} and \textbf{b} are vectors. The Jacobi method is an iterative scheme where we make a guess for the unknown, \textbf{x}, and after $k+1$ iterations we have

\begin{equation}
\textbf{x}^{(k+1)}=\hat{D}^{-1}(\textbf{b} - (\hat{L} + \hat{U}) \textbf{x}^{(k)})
\end{equation} 

where we have defined $\hat{A} = \hat{D} + \hat{U} + \hat{L}$. $\hat{D}$ is the diagonal matrix, $\hat{U}$ is an upper triangular matrix, and $\hat{L}$ is a lower triangular matrix.\cite{Jensen} 

\subsection{Eigenvalue problems}
Given the eigenvalue problem 

\begin{equation} \label{eigenvalue}
\textbf{A}\textbf{x}^{(v)}=\lambda^{(v)}\textbf{x}^{(v)}
\end{equation}

where \textbf{A} is a matrix of dimension $n$. We also have that $\lambda^{(v)}$ are the eigenvalues and $\textbf{x}^{(v)}$ the corresponding eigenvectors. From this we find that the eigenvalues of \textbf{A} are given by the $n$ roots of the characteristic polynomial given by:

\begin{equation}
P(\lambda)=\det(\lambda \textbf{I} - \textbf{A}) = \prod^{n}_{i=1}(\lambda_{i} - \lambda)
\end{equation}

This procedure is only valid for problems where we only need a small fraction of the eigenvalues and eigenvectors, or if the matrix is on a tridiagonal form. As our matrix is tridiagonal we can use the above procedure which leads us to the Jacobi method. A general procedure for solving equation \eqref{eigenvalue} is to perform similarity transformations until the original matrix \textbf{A} is either on diagonal form or is a tridiagonal matrix which then easily can be diagonalized. The general procedure leads to the well known Householder's algorithm.\cite{Jensen}


\subsection{Jacobi's method}
Given an $n \times n$ orthogonal transformation matrix

\begin{equation}
\textbf{S} =
\begin{pmatrix}
1 & 0 & \hdots & 0 & 0 & \hdots & 0 & 0 \\
0 & 1 & \hdots & 0 & 0 & \hdots & 0 & 0 \\
\hdots & \hdots & \hdots & \hdots & \hdots & \hdots & 0 & \hdots \\ 
0 & 0 & \hdots & \cos \theta & 0 & \hdots & 0 & \sin \theta \\
0 & 0 & \hdots & 0 & 1 & \hdots & 0 & 0 \\
\hdots & \hdots & \hdots & \hdots & \hdots & \hdots & 0 & \hdots \\
0 & 0 & \hdots & 0 & 0 & \hdots & 1 & 0 \\ 
0 & 0 & \hdots & \sin \theta & \hdots & \hdots & 0 & \cos \theta \\
\end{pmatrix}
\end{equation}

with the property $\textbf{S}^{\textbf{T}}=\textbf{S}^{-\textbf{1}}$. This matrix performs a plane rotation around an angle $\theta$ in the Euclidean $n$-dimensional space. Which means that the non-zero matrix elements are given by
\begin{equation}
s_{kk}=s_{ll}=\cos \theta, \:\: s_{kl}=-s_{lk}=-\sin \theta, \:\: s_{ii}=-s_{ii}=1 \quad i \neq k \: \: i \neq l,
\end{equation}

A similarity transformation 

\begin{equation}
\textbf{B}=\textbf{S}_{u\textbf{T}} \textbf{A} \textbf{S},
\end{equation}

results in 

\begin{flalign*}
b_{ii} &= a_{ii}, \: i \neq k, i\neq l 
\\
b_{ik} &= a_{ik} \cos \theta - a_{il} \sin \theta, \: i \neq k, i\neq l 
\\
b_{il} &= a_{il} \cos \theta + a_{ik} \sin \theta, \: i \neq k, i\neq l 
\\
b_{kk} &= a_{kk} \cos^{2} \theta - 2 a_{kl} \cos \theta \, \sin \theta + 		a_{ll} \sin^{2} \theta 
\\
b_{ll} &= a_{all} \cos^{2} \theta + 2 a_{kl} \cos \theta \, \sin \theta + 		a_{kk} \sin^{2} \theta 
\\
b_{kl} &= (a_{kk} - a_{ll}) \cos \theta \, \sin \theta + a_{kl} (\cos^{2} \theta - sin^{2} \theta)
\end{flalign*}

We can choose the angle $\theta$ as we wish, while making sure that all the non-diagonal matrix elements, $b_{kl}$ become zero. The algorithm which follows is then quite simple. We do a number of similarity transformations until the sum over the squared non-diagonal matrix elements are less than some chosen tolerance, typically less than $10^{-8}$. We therefore seek to minimize

\begin{equation}
\text{off}(\textbf{A}) = \sqrt{\sum^{n}_{i=1} \sum^{n}_{j=1, \: j \neq i} a_{ij}^{2}}
\end{equation}



\subsection{Scaling equations}



\subsection{Implementation}

All our code, benchmark calculations, and plots used can be found in \href{https://github.com/auduntre/FYS4150/tree/master/Project%202}{Auduns GitHub}.

\section{Results}
\subsection{Preservation of dot product}
We want to show that an orthogonal transformation preserves the dot product and orthogonality. Given a basis of vectors $\textbf{v}_{i}$

\begin{equation}
\textbf{v}_{i}=
\begin{bmatrix}
v_{i1} \\
v_{i2} \\
\vdots \\
v_{in}
\end{bmatrix}
\end{equation}

And assuming that the basis is orthogonal.

\begin{equation}
\textbf{v}^{T}_{j} \textbf{v}_{i} = \delta_{ij}
\end{equation}

The transformation is given by

\begin{equation}
\textbf{w}_{i} = \textbf{U} \textbf{v}_{i}
\end{equation}

As the transformation matrix \textbf{U} is orthogonal we have $\textbf{U}^{T}\textbf{U} = \textbf{I}$.

\begin{align*}
\textbf{w}_{i} \cdot \textbf{w}_{j} & =\textbf{U} \textbf{v}_{j} \cdot \textbf{U} \textbf{v}_{j} = (\textbf{U}\textbf{v}_{i})^{T}(\textbf{U}\textbf{v}_{j})=(\textbf{U}^{T} \textbf{v}_{i}^{T})(\textbf{U}\textbf{v}_{j})\\
&= \textbf{v}_{i}^{T} (\textbf{U}^{T} \textbf{U}) \textbf{v}_{j}= \textbf{v}_{i}^{T} \, \textbf{I} \, \textbf{v}_{j} =  \textbf{v}_{i}^{T} \,  \textbf{v}_{j} = \textbf{v}_{i} \cdot \textbf{v}_{j}
\end{align*}




\section{Discussion}
\subsection{Jacobi method}
The Jacobi method is generally a slower algorithm than those based tridiagonalization. However the Jacobi method is easy to parallelize which can improve performance significantly. The main reason for the Jacobi methods slow convergence is that for each new rotation, matrix elements which were zero might change to non-zero values.


\section{Appendix A}

\newpage

\bibliography{references}
\end{document}